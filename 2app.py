# app.py (Accelerated Version)

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import time
import asyncio # <<< MODIFIED: å¼•å…¥ asyncio
import openai # <<< MODIFIED: ç¢ºä¿ openai å·²å¼•å…¥

# ========== 1. YouTube Search (No changes) ==========
def search_youtube_videos(keywords, youtube_client, max_per_keyword, start_date, end_date):
    all_video_ids = set()
    for query in keywords:
        try:
            search_response = youtube_client.search().list(
                q=query,
                part='id,snippet',
                type='video',
                maxResults=max_per_keyword,
                publishedAfter=f"{start_date}T00:00:00Z",
                publishedBefore=f"{end_date}T23:59:59Z",
                relevanceLanguage='zh-Hant',
                regionCode='HK'
            ).execute()
            video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
            all_video_ids.update(video_ids)
            time.sleep(0.5) # Keep a small delay to be nice to YouTube API
        except Exception as e:
            st.warning(f"æœå°‹é—œéµå­— '{query}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    return list(all_video_ids)

# ========== 2. Batch Fetch Comments (No changes) ==========
def get_all_comments(video_ids, youtube_client, max_per_video):
    all_comments = []
    for video_id in video_ids:
        try:
            request = youtube_client.commentThreads().list(
                part='snippet', videoId=video_id, textFormat='plainText', maxResults=100
            )
            comments_fetched = 0
            while request and comments_fetched < max_per_video:
                response = request.execute()
                for item in response['items']:
                    if comments_fetched >= max_per_video:
                        break
                    comment = item['snippet']['topLevelComment']['snippet']
                    all_comments.append({
                        'video_id': video_id,
                        'comment_text': comment['textDisplay'],
                        'published_at': comment['publishedAt'],
                        'like_count': comment['likeCount']
                    })
                    comments_fetched += 1
                if comments_fetched >= max_per_video:
                    break
                request = youtube_client.commentThreads().list_next(request, response)
        except Exception as e:
            # st.warning(f"æŠ“å–å½±ç‰‡ ID '{video_id}' çš„ç•™è¨€æ™‚ç™¼ç”ŸéŒ¯èª¤ (å¯èƒ½å·²é—œé–‰ç•™è¨€): {e}")
            continue
    return pd.DataFrame(all_comments)

# ========== 3. DeepSeek AI Sentiment Analysis (MODIFIED FOR ASYNC) ==========
async def analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore, max_retries=3): # <<< MODIFIED
    import json
    if not isinstance(comment_text, str) or len(comment_text.strip()) < 5:
        return {"sentiment": "Invalid", "topic": "N/A", "summary": "Comment too short or invalid."}
    
    system_prompt = (
        "You are a professional Hong Kong market sentiment analyst. "
        "Analyze the following movie comment and strictly return the result in JSON format. "
        "The JSON object must contain three keys: "
        "1. 'sentiment': Must be either 'Positive', 'Negative', or 'Neutral'. "
        "2. 'topic': The core topic of the comment, e.g., 'Plot', 'Acting', 'Action Design', "
        "'Visuals', 'Pace', or 'Overall'. If unable to determine, use 'N/A'. "
        "3. 'summary': A concise one-sentence summary of the comment's main point. "
        "Ensure the output is only the JSON object and nothing else."
    )
    
    async with semaphore: # <<< MODIFIED: æ§åˆ¶ä¸¦ç™¼æ•¸é‡
        for attempt in range(max_retries):
            try:
                # <<< MODIFIED: ä½¿ç”¨ await å’Œç•°æ­¥ client
                response = await deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": comment_text}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.1,
                )
                analysis_result = json.loads(response.choices[0].message.content)
                return analysis_result
            except Exception as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt) # <<< MODIFIED: ä½¿ç”¨ asyncio.sleep
                else:
                    return {"sentiment": "Error", "topic": "Error", "summary": f"API Error: {e}"}

# ========== 4. Main Process (MODIFIED FOR ASYNC) ==========
async def run_all_analyses(df, deepseek_client): # <<< MODIFIED: æ–°å¢çš„ç•°æ­¥ä»»å‹™çµ±ç±Œå‡½æ•¸
    # æ§åˆ¶ä¸¦ç™¼æ•¸é‡ï¼Œé¿å…ç¬é–“è«‹æ±‚éå¤šå°è‡´ API æ‹’çµ•ã€‚å¯æ ¹æ“š API rate limit èª¿æ•´ã€‚
    semaphore = asyncio.Semaphore(50) 
    
    tasks = []
    for comment_text in df['comment_text']:
        tasks.append(analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore))
        
    # ä½¿ç”¨ tqdm ä¾†é¡¯ç¤ºé€²åº¦æ¢
    from tqdm.asyncio import tqdm_asyncio
    results = await tqdm_asyncio.gather(*tasks, desc="AI Sentiment Analysis (Concurrent)")
    return results

def movie_comment_analysis(
    movie_title, start_date, end_date,
    yt_api_key, deepseek_api_key,
    max_videos_per_keyword=30, max_comments_per_video=50, sample_size=None
):
    # Keywords
    SEARCH_KEYWORDS = [
        f'"{movie_title}" é å‘Š', f'"{movie_title}" review', f'"{movie_title}" å½±è©•',
        f'"{movie_title}" åˆ†æ', f'"{movie_title}" å¥½å””å¥½ç‡', f'"{movie_title}" è¨è«–',
        f'"{movie_title}" reaction'
    ]
    # API init
    from googleapiclient.discovery import build
    youtube_client = build('youtube', 'v3', developerKey=yt_api_key)
    
    # <<< MODIFIED: åˆå§‹åŒ– Async Client
    deepseek_client = openai.AsyncOpenAI(
        api_key=deepseek_api_key,
        base_url="https://api.deepseek.com/v1"
    )

    # Search videos
    video_ids = search_youtube_videos(
        SEARCH_KEYWORDS, youtube_client, max_videos_per_keyword, start_date, end_date
    )
    if not video_ids:
        return None, "æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ã€‚"
    # Fetch comments
    df_comments = get_all_comments(video_ids, youtube_client, max_comments_per_video)
    if df_comments.empty:
        return None, "æ‰¾ä¸åˆ°ä»»ä½•ç•™è¨€ã€‚"
    # Time processing
    df_comments['published_at'] = pd.to_datetime(df_comments['published_at'], utc=True)
    df_comments['published_at_hk'] = df_comments['published_at'].dt.tz_convert('Asia/Hong_Kong')
    # Filter by HK timezone
    start = pd.to_datetime(start_date).tz_localize('Asia/Hong_Kong')
    end = pd.to_datetime(end_date).tz_localize('Asia/Hong_Kong') + timedelta(days=1)
    mask = (df_comments['published_at_hk'] >= start) & (df_comments['published_at_hk'] <= end)
    df_comments = df_comments.loc[mask].reset_index(drop=True)
    if df_comments.empty:
        return None, "åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§æ²’æœ‰ç•™è¨€ã€‚"
    # Sampling
    if sample_size and sample_size > 0 and sample_size < len(df_comments):
        df_analyze = df_comments.sample(n=sample_size, random_state=42)
    else:
        df_analyze = df_comments

    # <<< MODIFIED: åŸ·è¡Œç•°æ­¥åˆ†æ
    # ç§»é™¤ tqdm.pandasï¼Œå› ç‚ºæˆ‘å€‘ç”¨è‡ªå·±çš„ç•°æ­¥é€²åº¦æ¢
    st.info(f"æº–å‚™å° {len(df_analyze)} å‰‡ç•™è¨€é€²è¡Œé«˜é€Ÿä¸¦ç™¼åˆ†æ...")
    analysis_results = asyncio.run(run_all_analyses(df_analyze, deepseek_client))
    
    analysis_df = pd.json_normalize(analysis_results)
    final_df = pd.concat([df_analyze.reset_index(drop=True), analysis_df], axis=1)
    final_df['published_at'] = pd.to_datetime(final_df['published_at'])
    return final_df, None

# ========== 5. Streamlit UI (MODIFIED FOR PROGRESS BAR) ==========
st.set_page_config(page_title="YouTube é›»å½±è©•è«– AI åˆ†æ", layout="wide")
st.title("ğŸ¬ YouTube é›»å½±è©•è«– AI æƒ…æ„Ÿåˆ†æ")

with st.expander("ä½¿ç”¨èªªæ˜"):
    st.markdown("""
    1.  è¼¸å…¥é›»å½±çš„**ä¸­æ–‡å…¨å**ã€åˆ†ææ™‚é–“ç¯„åœåŠæ‰€éœ€çš„ API é‡‘é‘°ã€‚
    2.  è‡ªè¨‚æ¯å€‹é—œéµå­—æœå°‹çš„å½±ç‰‡æ•¸é‡ä¸Šé™ï¼ŒåŠæ¯éƒ¨å½±ç‰‡æŠ“å–çš„ç•™è¨€æ•¸é‡ä¸Šé™ã€‚
    3.  é»æ“Šã€Œé–‹å§‹åˆ†æã€ï¼Œç³»çµ±å°‡è‡ªå‹•æŠ“å– YouTube ç•™è¨€ä¸¦é€²è¡Œ AI é«˜é€Ÿæƒ…æ„Ÿåˆ†æã€‚
    4.  åˆ†æå®Œæˆå¾Œï¼Œä¸‹æ–¹æœƒé¡¯ç¤ºæ•¸æ“šåœ–è¡¨åŠè©³ç´°çµæœçš„ä¸‹è¼‰æŒ‰éˆ•ã€‚
    """)

movie_title = st.text_input("é›»å½±åç¨± (å»ºè­°ä½¿ç”¨é¦™æ¸¯é€šç”¨çš„ä¸­æ–‡å…¨å)", value="ä¹é¾åŸå¯¨ä¹‹åœåŸ")
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("é–‹å§‹æ—¥æœŸ", value=datetime.today() - timedelta(days=30))
with col2:
    end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.today())
yt_api_key = st.text_input("YouTube API Key", type='password')
deepseek_api_key = st.text_input("DeepSeek API Key", type='password')

st.subheader("é€²éšè¨­å®š")
max_videos = st.slider("æ¯å€‹é—œéµå­—çš„æœ€å¤§å½±ç‰‡æœå°‹æ•¸", 5, 50, 10, help="å¢åŠ æ­¤æ•¸å€¼æœƒæ‰¾åˆ°æ›´å¤šå½±ç‰‡ï¼Œä½†æœƒå¢åŠ  YouTube API çš„é…é¡æ¶ˆè€—ã€‚")
max_comments = st.slider("æ¯éƒ¨å½±ç‰‡çš„æœ€å¤§ç•™è¨€æŠ“å–æ•¸", 10, 200, 50, help="åˆ†æçš„ä¸»è¦ä¾†æºï¼Œæ•¸é‡è¶Šå¤šï¼Œåˆ†æçµæœè¶Šå…¨é¢ï¼Œä½† DeepSeek API æˆæœ¬è¶Šé«˜ã€‚")
sample_size = st.number_input("åˆ†æç•™è¨€æ•¸é‡ä¸Šé™ (0 ä»£è¡¨åˆ†æå…¨éƒ¨å·²æŠ“å–çš„ç•™è¨€)", 0, 5000, 500, help="è¨­å®šä¸€å€‹ä¸Šé™ä»¥æ§åˆ¶åˆ†ææ™‚é–“å’Œæˆæœ¬ã€‚ä¾‹å¦‚ï¼Œå³ä½¿æŠ“å–äº† 2000 å‰‡ç•™è¨€ï¼Œé€™è£¡è¨­ 500 å°±åªæœƒåˆ†æå…¶ä¸­çš„ 500 å‰‡ã€‚")

if st.button("ğŸš€ é–‹å§‹åˆ†æ"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.warning("è«‹å¡«å¯«é›»å½±åç¨±å’Œå…©å€‹ API é‡‘é‘°ã€‚")
    else:
        # <<< MODIFIED: ç§»é™¤ with st.spinnerï¼Œå› ç‚ºæˆ‘å€‘åœ¨å‡½æ•¸å…§æœ‰è‡ªå·±çš„é€²åº¦æç¤º
        # æˆ‘å€‘éœ€è¦ä¸€å€‹ placeholder ä¾†é¡¯ç¤º tqdm çš„é€²åº¦æ¢
        progress_placeholder = st.empty()
        
        # ç”±æ–¼ Streamlit çš„é™åˆ¶ï¼Œç›´æ¥åœ¨ UI é¡¯ç¤º tqdm é€²åº¦æ¢æ¯”è¼ƒå›°é›£
        # æˆ‘å€‘æ”¹ç‚ºåœ¨å¾Œç«¯æ‰“å°ï¼Œä¸¦åœ¨å‰ç«¯é¡¯ç¤ºä¸€å€‹é€šç”¨çš„ spinner
        with st.spinner("AI é«˜é€Ÿåˆ†æä¸­... (è™•ç† 500 å‰‡ç•™è¨€ç´„éœ€ 1-2 åˆ†é˜)"):
            df_result, err = movie_comment_analysis(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size
            )
        
        if err:
            st.error(err)
        else:
            st.success("åˆ†æå®Œæˆï¼")
            st.dataframe(df_result.head(20))

            # ========== å¯è§†åŒ– (No changes) ==========
            st.subheader("1. æƒ…æ„Ÿåˆ†ä½ˆåœ“é¤…åœ–")
            fig1, ax1 = plt.subplots(figsize=(5, 4))
            valmap = {
                "Positive": "æ­£é¢", "Negative": "è² é¢", "Neutral": "ä¸­æ€§",
                "Invalid": "ç„¡æ•ˆ", "Error": "éŒ¯èª¤"
            }
            # ç¢ºä¿æ‰€æœ‰å¯èƒ½çš„å€¼éƒ½å­˜åœ¨ï¼Œé¿å… Key-Error
            df_result['sentiment_cn'] = df_result['sentiment'].map(lambda x: valmap.get(str(x), str(x)))
            
            # å®šç¾©é¡è‰²å’Œé †åºï¼Œç¢ºä¿åœ–è¡¨ä¸€è‡´æ€§
            s_counts = df_result['sentiment_cn'].value_counts()
            order = ['æ­£é¢', 'è² é¢', 'ä¸­æ€§', 'ç„¡æ•ˆ', 'éŒ¯èª¤']
            colors_map = {'æ­£é¢': '#5cb85c', 'è² é¢': '#d9534f', 'ä¸­æ€§': '#f0ad4e', 'ç„¡æ•ˆ': '#cccccc', 'éŒ¯èª¤': '#888888'}
            
            # éæ¿¾æ‰ä¸å­˜åœ¨çš„æ¨™ç±¤
            s_counts = s_counts.reindex(order).dropna()
            
            s_counts.plot.pie(
                autopct='%.1f%%', ax=ax1,
                colors=[colors_map[key] for key in s_counts.index],
                wedgeprops={'linewidth': 1.0, 'edgecolor': 'white'}
            )
            ax1.set_title('æ•´é«”æƒ…æ„Ÿåˆ†ä½ˆ', fontsize=16)
            ax1.set_ylabel('')
            st.pyplot(fig1, use_container_width=False)

            st.subheader("2. æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢ (å †ç–Šé•·æ¢åœ–)")
            df_result['date'] = df_result['published_at_hk'].dt.date
            daily = df_result.groupby(['date', 'sentiment_cn']).size().unstack().fillna(0)
            
            # ç¢ºä¿æ¬„ä½é †åº
            daily = daily.reindex(columns=order).dropna(axis=1, how='all')

            fig2, ax2 = plt.subplots(figsize=(10, 4))
            daily.plot(kind='bar', stacked=True, ax=ax2, width=0.8, 
                       color=[colors_map[col] for col in daily.columns])
            ax2.set_title('æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢', fontsize=16)
            ax2.set_xlabel('æ—¥æœŸ')
            ax2.set_ylabel('ç•™è¨€æ•¸é‡')
            plt.xticks(rotation=45, ha='right')
            plt.legend(title='æƒ…æ„Ÿ')
            plt.tight_layout()
            st.pyplot(fig2, use_container_width=True)

            st.subheader("3. å„ä¸»é¡Œæƒ…æ„Ÿä½”æ¯”")
            topic_sentiment = df_result.groupby(['topic', 'sentiment_cn']).size().unstack().fillna(0)
            topic_sentiment = topic_sentiment.reindex(columns=order).dropna(axis=1, how='all')
            
            # è¨ˆç®—ç™¾åˆ†æ¯”
            topic_sentiment_percent = topic_sentiment.div(topic_sentiment.sum(axis=1), axis=0) * 100

            fig3, ax3 = plt.subplots(figsize=(10, 5))
            topic_sentiment_percent.plot(kind='bar', stacked=True, ax=ax3,
                                         color=[colors_map[col] for col in topic_sentiment_percent.columns])
            ax3.set_title('å„è¨è«–ä¸»é¡Œçš„æƒ…æ„Ÿä½”æ¯”', fontsize=16)
            ax3.set_xlabel('ä¸»é¡Œ')
            ax3.set_ylabel('ç™¾åˆ†æ¯” (%)')
            ax3.yaxis.set_major_formatter(plt.FuncFormatter('{:.0f}%'.format))
            plt.xticks(rotation=45, ha='right')
            plt.legend(title='æƒ…æ„Ÿ')
            plt.tight_layout()
            st.pyplot(fig3, use_container_width=True)

            st.subheader("4. ä¸‹è¼‰åˆ†ææ˜ç´°")
            csv = df_result.to_csv(index=False, encoding='utf-8-sig')
            st.download_button("ğŸ“¥ ä¸‹è¼‰å…¨éƒ¨åˆ†ææ˜ç´° (CSV)", csv, file_name=f"{movie_title}_analysis_details.csv", mime='text/csv')